{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154cdce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#task 3\n",
    "#Even when it seems as the shortest part of this lab, it took a lot of time to understand, how to launch pipeline\n",
    "#!pip install ipywidgets\n",
    "#!pip install transformers\n",
    "#!pip3 install PyJWT\n",
    "#!pip install 'h5py==2.10.0' --force-reinstall\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "#!pip install tensorflow\n",
    "#!pip install google-colab\n",
    "import codecs\n",
    "from transformers import pipeline\n",
    "from ipywidgets import FloatProgress\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65796f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e70a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996846318244934}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_one = 'Give me A please'\n",
    "classifier(text_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8794c7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9926418662071228}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_two = 'Or Give me B please :)'\n",
    "classifier(text_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b98f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9932469725608826}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_three = 'I am Oleksii Chalyi'\n",
    "classifier(text_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e2ccad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/kali/anaconda3/envs/teststackoverflow/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Sure', 'end': 3, 'score': 0.6166552305221558, 'start': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_qusetion_answering = pipeline('question-answering')\n",
    "nlp_qusetion_answering(context='Sure', question='Will I take A in Data Analysis?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92935423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e39e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2507, 2033, 1037, 3531, 102], [101, 2030, 2507, 2033, 1038, 3531, 1024, 1007, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([text_one, text_two])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "461a6b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
       "array([[ 101, 2507, 2033, 1037, 3531,  102,    0,    0,    0],\n",
       "       [ 101, 2030, 2507, 2033, 1038, 3531, 1024, 1007,  102]],\n",
       "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_padding = tokenizer([text_one, text_two], padding = True, truncation = True, max_length = 256, return_tensors=\"tf\")\n",
    "inputs_with_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2f6740f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[-3.8784578,  4.1830215],\n",
       "        [-2.4131548,  2.4914017]], dtype=float32)>,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs_with_padding)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "586dc6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[3.1536023e-04, 9.9968469e-01],\n",
       "       [7.3581878e-03, 9.9264187e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tf.nn.softmax(outputs[0], axis=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78259718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2507, 2033, 1037, 3531, 102], [101, 2030, 2507, 2033, 1038, 3531, 1024, 1007, 102], [101, 1045, 2572, 15589, 5705, 6137, 15775, 2135, 2072, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([text_one, text_two, text_three])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f6bacd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 10), dtype=int32, numpy=\n",
       "array([[  101,  2507,  2033,  1037,  3531,   102,     0,     0,     0,\n",
       "            0],\n",
       "       [  101,  2030,  2507,  2033,  1038,  3531,  1024,  1007,   102,\n",
       "            0],\n",
       "       [  101,  1045,  2572, 15589,  5705,  6137, 15775,  2135,  2072,\n",
       "          102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 10), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_padding = tokenizer([text_one, text_two, text_three], padding = True, truncation = True, max_length = 256, return_tensors=\"tf\")\n",
    "inputs_with_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f0b6cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[-3.8784575,  4.1830215],\n",
       "        [-2.413155 ,  2.491402 ],\n",
       "        [-2.426911 ,  2.5640795]], dtype=float32)>,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs_with_padding)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03f1a98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[3.1536023e-04, 9.9968469e-01],\n",
       "       [7.3581808e-03, 9.9264187e-01],\n",
       "       [6.7530121e-03, 9.9324691e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tf.nn.softmax(outputs[0], axis=-1)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
